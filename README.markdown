# ğŸš€ Balancing Act: Mastering CartPole with Reinforcement Learning ğŸ¤–

Welcome to **CartPole RL Mastery**, an electrifying journey into the world of reinforcement learning (RL) where a clever AI learns to balance a pole on a moving cart like a digital acrobat! This project showcases the power of the Proximal Policy Optimization (PPO) algorithm using the **CartPole-v1** environment from Gymnasium, brought to life with vibrant visualizations and a polished video output. Buckle up for a thrilling ride through code, learning, and dynamic simulations! ğŸ¢

---

## ğŸŒŸ Project Highlights

- **Dynamic Visualizations**: Watch the AI learn in real-time with a custom Matplotlib visualization, complete with a cart, wheels, and a pole that dances to the rhythm of RL.
- **Proximal Policy Optimization (PPO)**: Harness the power of Stable-Baselines3â€™s PPO algorithm to train a robust agent that masters the art of balance.
- **Video Magic**: Capture the training and testing phases as MP4 videos, showcasing the AIâ€™s progress from wobbly beginner to pole-balancing pro.
- **Insightful Metrics**: Track the agentâ€™s learning curve with a sleek reward-over-time plot, revealing the journey to mastery.
- **Ready-to-Run**: A complete Jupyter Notebook with clear, modular code, perfect for learning, tweaking, and extending.

---

## ğŸ® What is CartPole?

Imagine a circus act where a cart zips left and right to keep a pole upright. Thatâ€™s **CartPole**! The agent observes the cartâ€™s position, velocity, pole angle, and angular velocity, deciding whether to push left or right to maximize balance time. Itâ€™s a classic RL problem thatâ€™s simple yet profound, teaching the AI to make split-second decisions in a dynamic environment.

---

## ğŸ› ï¸ How It Works

This project is a symphony of code and creativity, orchestrated in a Jupyter Notebook. Hereâ€™s the breakdown:

1. **Environment Setup**: We initialize the CartPole-v1 environment from Gymnasium, rendering it as RGB frames for visualization.
2. **PPO Model**: A PPO model with a multilayer perceptron (MLP) policy learns to balance the pole through trial and error.
3. **Custom Visualization**: A bespoke `VisualCallback` class paints a vivid picture of the cart and pole, updating in real-time with state info (cart position, pole angle, and rewards).
4. **Training & Evaluation**: The agent trains for 10,000 timesteps, with periodic evaluations to track performance and save the best model.
5. **Testing Showcase**: Load the trained model and watch it perform, with frames saved as a mesmerizing test video.
6. **Reward Plot**: A sleek Matplotlib plot visualizes the mean reward over time, highlighting the AIâ€™s learning curve.

---

## ğŸš€ Getting Started

Ready to dive into the balancing act? Follow these steps to run the project locally or in a cloud environment like Google Colab.

### Prerequisites
- Python 3.8+
- Install dependencies:
  ```bash
  pip install gymnasium stable-baselines3 matplotlib imageio numpy
  ```
Open the Jupyter Notebook:
   ```bash
   jupyter notebook Project_2_Reinforcement_Learning_for_Simple_Game_Environments_CartPole.ipynb
   ```
Run all cells to train the model, visualize the process, and generate videos (`cartpole_training.mp4` and `cartpole_test.mp4`).
Download the trained model (`ppo_cartpole.zip`) and videos for further analysis or sharing.

### Outputs
- **Videos**: `cartpole_training.mp4` (training process) and `cartpole_test.mp4` (trained agent performance).
- **Model**: `ppo_cartpole.zip` for reloading and testing.
- **Plot**: A reward-over-time graph showing the agentâ€™s learning progress.

---

## ğŸ¨ Visual Feast

The project doesnâ€™t just train an AIâ€”it brings the process to life! The custom visualization includes:
- A **blue cart** with realistic wheels, zipping along a track.
- A **red pole** with a yellow tip, swaying as the AI learns to balance it.
- A **real-time dashboard** displaying step count, cart position, pole angle, and episode reward.
- A smooth MP4 video capturing every moment of the training and testing phases.

Check out the videos in the `outputs/` folder to witness the AIâ€™s journey from novice to expert!

---

## ğŸŒ Why This Matters

This project is more than codeâ€”itâ€™s a gateway to understanding reinforcement learning in action. By mastering CartPole, youâ€™ll gain insights into:
- **RL Fundamentals**: Learn how agents interact with environments to maximize rewards.
- **PPO Power**: Explore why PPO is a go-to algorithm for stable and efficient RL training.
- **Visualization Skills**: See how to create engaging, informative visualizations for AI projects.
- **Real-World Applications**: From robotics to game AI, the principles here apply to countless domains.

Whether youâ€™re a student, hobbyist, or AI enthusiast, this project is your launchpad to RL greatness! ğŸš€

---

## ğŸ”§ Extending the Project

Want to take it further? Here are some ideas to remix this project:
- **Tweak Hyperparameters**: Adjust PPOâ€™s learning rate or network architecture for better performance.
- **Try Other Environments**: Swap CartPole for Gymnasiumâ€™s LunarLander or MountainCar.
- **Enhance Visuals**: Add animations for rewards or color-code the pole based on stability.
- **Compare Algorithms**: Test DQN or A2C against PPO to see which reigns supreme.
- **Deploy It**: Convert the model to a web app using Flask or Streamlit for interactive demos.

---

## ğŸ“‚ Repository Structure

```
cartpole-rl-mastery/
â”œâ”€â”€ Project_2_Reinforcement_Learning_for_Simple_Game_Environments_CartPole.ipynb
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ ppo_cartpole.zip
â”‚   â”œâ”€â”€ cartpole_training.mp4
â”‚   â”œâ”€â”€ cartpole_test.mp4
â”œâ”€â”€ README.md
```

---

## ğŸ¤ Contributing

Got ideas to make this project even cooler? Contributions are welcome! Fork the repo, make your changes, and submit a pull request. Letâ€™s build something epic together! ğŸ‰

---

## ğŸ“œ License

This project is licensed under the MIT Licenseâ€”feel free to use, share, and modify it with attribution.

---

## ğŸŒŸ Acknowledgments

- **Gymnasium**: For the CartPole environment that makes RL so accessible.
- **Stable-Baselines3**: For a rock-solid PPO implementation.
- **Matplotlib & Imageio**: For bringing the AIâ€™s journey to life with stunning visuals.
- **You**: For exploring this project and joining the RL adventure!

---

**Ready to balance the pole and conquer RL?** Run the notebook, watch the AI learn, and share your results! Letâ€™s make AI magic happen! âœ¨
